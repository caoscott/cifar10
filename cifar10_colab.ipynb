{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10-colab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caoscott/cifar10/blob/master/cifar10_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GtKqE0hAcYNT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import transforms\n",
        "\n",
        "train_preprocess = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])\n",
        "\n",
        "test_preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngTUVSxickTE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 60
        },
        "outputId": "e89f8977-ad2e-4222-ce4b-2bf9d453906f"
      },
      "source": [
        "from torchvision import datasets\n",
        "\n",
        "cifar10_train = datasets.CIFAR10('data', train=True, download=True,\n",
        "                                 transform=train_preprocess)\n",
        "cifar10_test = datasets.CIFAR10('data', train=False, download=True,\n",
        "                                transform=test_preprocess)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oKYuTwfyerPP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "train_loader = data.DataLoader(cifar10_train, batch_size=512, \n",
        "                               num_workers=3, shuffle=True, drop_last=True)\n",
        "test_loader = data.DataLoader(cifar10_test, batch_size=2048, \n",
        "                              num_workers=3, shuffle=False, drop_last=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdqlQjXbfIBN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class Pass(nn.Module):\n",
        "  def forward(self, x):\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TXpX6UeqfL65",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torchvision import models\n",
        "\n",
        "model = models.resnet152()\n",
        "# model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)\n",
        "model.maxpool = Pass()\n",
        "model.fc = nn.Linear(2048, 10)\n",
        "model = model.cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8p_22kWlfi9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import optim\n",
        "from torch.optim import lr_scheduler\n",
        "\n",
        "loss_fn = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\n",
        "                      weight_decay=5e-4, nesterov=True)\n",
        "lr_schedule = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, \n",
        "                                             verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nv7_ZeM_r9O3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def acc(loader):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for x, y in loader:\n",
        "      x, y = x.cuda(), y.cuda()\n",
        "      scores = model(x)\n",
        "      _, predicted = torch.max(scores, 1)\n",
        "      total += y.size(0)\n",
        "      correct += predicted.eq(y).sum().item()\n",
        "    return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hne5ctuoW7f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4758
        },
        "outputId": "a3a18c86-1715-4fd4-b522-1cc35dfc8b31"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "  batch_time = time.time()\n",
        "  epoch_time = batch_time\n",
        "  for iteration, (x, y) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    x, y = x.cuda(), y.cuda()\n",
        "    scores = model(x)\n",
        "    loss = loss_fn(scores, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      _, predicted = torch.max(scores, 1)\n",
        "      train_acc = torch.eq(predicted, y).sum().item()/x.shape[0]\n",
        "\n",
        "      print('\\r| Epoch {:3d} Iter {:3d} Time {:.3f} Avg Time {:.3f}\\t'\n",
        "            'Batch Loss: {:.4f} Batch Acc {:.3f}'.format(\n",
        "                epoch, iteration+1, time.time()-batch_time, \n",
        "                (time.time()-epoch_time)/(iteration+1),\n",
        "                loss.item(), train_acc\n",
        "            ), end='')\n",
        "      batch_time = time.time()\n",
        "      del x, y, scores, loss, predicted\n",
        "  optimizer.zero_grad()\n",
        "  test_acc = acc(test_loader)\n",
        "  lr_schedule.step(test_acc)\n",
        "  print('\\n| Epoch {:3d} Time {:.3f} \\tTest Acc {:.3f}'.format(\n",
        "    epoch, time.time()-epoch_time, test_acc\n",
        "  ))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   0 Iter  97 Time 1.267 Avg Time 1.274\tBatch Loss: 2.3084 Batch Acc 0.113\n",
            "| Epoch   0 Time 131.935 \tTest Acc 0.121\n",
            "| Epoch   1 Iter  97 Time 1.255 Avg Time 1.266\tBatch Loss: 2.0116 Batch Acc 0.219\n",
            "| Epoch   1 Time 131.063 \tTest Acc 0.208\n",
            "| Epoch   2 Iter  97 Time 1.261 Avg Time 1.268\tBatch Loss: 1.9492 Batch Acc 0.232\n",
            "| Epoch   2 Time 131.144 \tTest Acc 0.275\n",
            "| Epoch   3 Iter  97 Time 1.263 Avg Time 1.267\tBatch Loss: 1.6992 Batch Acc 0.334\n",
            "| Epoch   3 Time 131.388 \tTest Acc 0.334\n",
            "| Epoch   4 Iter  97 Time 1.265 Avg Time 1.269\tBatch Loss: 1.5954 Batch Acc 0.416\n",
            "| Epoch   4 Time 131.396 \tTest Acc 0.399\n",
            "| Epoch   5 Iter  97 Time 1.263 Avg Time 1.271\tBatch Loss: 1.5264 Batch Acc 0.441\n",
            "| Epoch   5 Time 131.730 \tTest Acc 0.427\n",
            "| Epoch   6 Iter  97 Time 1.261 Avg Time 1.271\tBatch Loss: 1.5779 Batch Acc 0.424\n",
            "| Epoch   6 Time 131.942 \tTest Acc 0.472\n",
            "| Epoch   7 Iter  97 Time 1.259 Avg Time 1.272\tBatch Loss: 1.3906 Batch Acc 0.480\n",
            "| Epoch   7 Time 131.603 \tTest Acc 0.502\n",
            "| Epoch   8 Iter  97 Time 1.264 Avg Time 1.272\tBatch Loss: 1.3056 Batch Acc 0.516\n",
            "| Epoch   8 Time 131.785 \tTest Acc 0.518\n",
            "| Epoch   9 Iter  97 Time 1.264 Avg Time 1.272\tBatch Loss: 1.3400 Batch Acc 0.502\n",
            "| Epoch   9 Time 131.755 \tTest Acc 0.530\n",
            "| Epoch  10 Iter  97 Time 1.261 Avg Time 1.274\tBatch Loss: 1.2461 Batch Acc 0.531\n",
            "| Epoch  10 Time 132.081 \tTest Acc 0.550\n",
            "| Epoch  11 Iter  97 Time 1.265 Avg Time 1.272\tBatch Loss: 1.2122 Batch Acc 0.559\n",
            "| Epoch  11 Time 131.839 \tTest Acc 0.577\n",
            "| Epoch  12 Iter  97 Time 1.267 Avg Time 1.274\tBatch Loss: 1.1270 Batch Acc 0.604\n",
            "| Epoch  12 Time 131.841 \tTest Acc 0.599\n",
            "| Epoch  13 Iter  97 Time 1.261 Avg Time 1.271\tBatch Loss: 1.0994 Batch Acc 0.611\n",
            "| Epoch  13 Time 131.616 \tTest Acc 0.622\n",
            "| Epoch  14 Iter  97 Time 1.267 Avg Time 1.272\tBatch Loss: 0.9853 Batch Acc 0.652\n",
            "| Epoch  14 Time 132.175 \tTest Acc 0.644\n",
            "| Epoch  15 Iter  97 Time 1.264 Avg Time 1.276\tBatch Loss: 0.9469 Batch Acc 0.674\n",
            "| Epoch  15 Time 132.260 \tTest Acc 0.659\n",
            "| Epoch  16 Iter  97 Time 1.266 Avg Time 1.275\tBatch Loss: 0.8899 Batch Acc 0.680\n",
            "| Epoch  16 Time 132.000 \tTest Acc 0.682\n",
            "| Epoch  17 Iter  97 Time 1.268 Avg Time 1.276\tBatch Loss: 0.7418 Batch Acc 0.768\n",
            "| Epoch  17 Time 132.349 \tTest Acc 0.681\n",
            "| Epoch  18 Iter  97 Time 1.266 Avg Time 1.274\tBatch Loss: 0.7548 Batch Acc 0.717\n",
            "| Epoch  18 Time 131.985 \tTest Acc 0.706\n",
            "| Epoch  19 Iter  97 Time 1.268 Avg Time 1.271\tBatch Loss: 0.7284 Batch Acc 0.736\n",
            "| Epoch  19 Time 131.726 \tTest Acc 0.720\n",
            "| Epoch  20 Iter  97 Time 1.261 Avg Time 1.273\tBatch Loss: 0.7314 Batch Acc 0.760\n",
            "| Epoch  20 Time 132.010 \tTest Acc 0.717\n",
            "| Epoch  21 Iter  97 Time 1.264 Avg Time 1.274\tBatch Loss: 0.7281 Batch Acc 0.740\n",
            "| Epoch  21 Time 132.031 \tTest Acc 0.743\n",
            "| Epoch  22 Iter  97 Time 1.265 Avg Time 1.272\tBatch Loss: 0.6022 Batch Acc 0.803\n",
            "| Epoch  22 Time 131.671 \tTest Acc 0.752\n",
            "| Epoch  23 Iter  97 Time 1.262 Avg Time 1.271\tBatch Loss: 0.5680 Batch Acc 0.807\n",
            "| Epoch  23 Time 131.711 \tTest Acc 0.756\n",
            "| Epoch  24 Iter  97 Time 1.266 Avg Time 1.274\tBatch Loss: 0.5763 Batch Acc 0.816\n",
            "| Epoch  24 Time 131.848 \tTest Acc 0.771\n",
            "| Epoch  25 Iter  97 Time 1.262 Avg Time 1.272\tBatch Loss: 0.6518 Batch Acc 0.771\n",
            "| Epoch  25 Time 131.674 \tTest Acc 0.771\n",
            "| Epoch  26 Iter  97 Time 1.262 Avg Time 1.273\tBatch Loss: 0.5810 Batch Acc 0.789\n",
            "| Epoch  26 Time 131.987 \tTest Acc 0.784\n",
            "| Epoch  27 Iter  97 Time 1.273 Avg Time 1.273\tBatch Loss: 0.4677 Batch Acc 0.836\n",
            "| Epoch  27 Time 132.035 \tTest Acc 0.788\n",
            "| Epoch  28 Iter  97 Time 1.263 Avg Time 1.273\tBatch Loss: 0.5399 Batch Acc 0.814\n",
            "| Epoch  28 Time 131.966 \tTest Acc 0.780\n",
            "| Epoch  29 Iter  97 Time 1.265 Avg Time 1.275\tBatch Loss: 0.5089 Batch Acc 0.834\n",
            "| Epoch  29 Time 131.849 \tTest Acc 0.801\n",
            "| Epoch  30 Iter  97 Time 1.266 Avg Time 1.273\tBatch Loss: 0.4563 Batch Acc 0.834\n",
            "| Epoch  30 Time 131.665 \tTest Acc 0.797\n",
            "| Epoch  31 Iter  97 Time 1.258 Avg Time 1.273\tBatch Loss: 0.4511 Batch Acc 0.838\n",
            "| Epoch  31 Time 131.993 \tTest Acc 0.806\n",
            "| Epoch  32 Iter  97 Time 1.260 Avg Time 1.274\tBatch Loss: 0.4017 Batch Acc 0.859\n",
            "| Epoch  32 Time 131.868 \tTest Acc 0.808\n",
            "| Epoch  33 Iter  97 Time 1.261 Avg Time 1.274\tBatch Loss: 0.5231 Batch Acc 0.828\n",
            "| Epoch  33 Time 132.050 \tTest Acc 0.803\n",
            "| Epoch  34 Iter  97 Time 1.265 Avg Time 1.275\tBatch Loss: 0.4683 Batch Acc 0.846\n",
            "| Epoch  34 Time 132.026 \tTest Acc 0.822\n",
            "| Epoch  35 Iter  97 Time 1.261 Avg Time 1.274\tBatch Loss: 1.1201 Batch Acc 0.613\n",
            "| Epoch  35 Time 131.908 \tTest Acc 0.623\n",
            "| Epoch  36 Iter  97 Time 1.265 Avg Time 1.272\tBatch Loss: 0.7217 Batch Acc 0.760\n",
            "| Epoch  36 Time 131.808 \tTest Acc 0.733\n",
            "| Epoch  37 Iter  97 Time 1.264 Avg Time 1.272\tBatch Loss: 0.6287 Batch Acc 0.775\n",
            "| Epoch  37 Time 131.631 \tTest Acc 0.764\n",
            "| Epoch  38 Iter  97 Time 1.259 Avg Time 1.271\tBatch Loss: 0.6278 Batch Acc 0.799\n",
            "| Epoch  38 Time 131.603 \tTest Acc 0.779\n",
            "| Epoch  39 Iter  97 Time 1.259 Avg Time 1.272\tBatch Loss: 0.5019 Batch Acc 0.822\n",
            "| Epoch  39 Time 131.999 \tTest Acc 0.787\n",
            "| Epoch  40 Iter  97 Time 1.259 Avg Time 1.271\tBatch Loss: 0.4369 Batch Acc 0.855\n",
            "| Epoch  40 Time 131.663 \tTest Acc 0.792\n",
            "| Epoch  41 Iter  97 Time 1.262 Avg Time 1.272\tBatch Loss: 0.4655 Batch Acc 0.857Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
            "\n",
            "| Epoch  41 Time 131.920 \tTest Acc 0.805\n",
            "| Epoch  42 Iter  97 Time 1.260 Avg Time 1.271\tBatch Loss: 0.2724 Batch Acc 0.904\n",
            "| Epoch  42 Time 131.767 \tTest Acc 0.837\n",
            "| Epoch  43 Iter  97 Time 1.259 Avg Time 1.270\tBatch Loss: 0.3022 Batch Acc 0.893\n",
            "| Epoch  43 Time 131.463 \tTest Acc 0.844\n",
            "| Epoch  44 Iter  97 Time 1.261 Avg Time 1.268\tBatch Loss: 0.2511 Batch Acc 0.910\n",
            "| Epoch  44 Time 131.384 \tTest Acc 0.843\n",
            "| Epoch  45 Iter  97 Time 1.258 Avg Time 1.271\tBatch Loss: 0.2133 Batch Acc 0.926\n",
            "| Epoch  45 Time 131.537 \tTest Acc 0.844\n",
            "| Epoch  46 Iter  97 Time 1.259 Avg Time 1.272\tBatch Loss: 0.1834 Batch Acc 0.930\n",
            "| Epoch  46 Time 131.706 \tTest Acc 0.842\n",
            "| Epoch  47 Iter  97 Time 1.256 Avg Time 1.270\tBatch Loss: 0.2078 Batch Acc 0.928\n",
            "| Epoch  47 Time 131.666 \tTest Acc 0.845\n",
            "| Epoch  48 Iter  97 Time 1.258 Avg Time 1.271\tBatch Loss: 0.2187 Batch Acc 0.922\n",
            "| Epoch  48 Time 131.727 \tTest Acc 0.847\n",
            "| Epoch  49 Iter  97 Time 1.266 Avg Time 1.270\tBatch Loss: 0.1902 Batch Acc 0.936\n",
            "| Epoch  49 Time 131.473 \tTest Acc 0.849\n",
            "| Epoch  50 Iter  97 Time 1.265 Avg Time 1.272\tBatch Loss: 0.1985 Batch Acc 0.924\n",
            "| Epoch  50 Time 131.850 \tTest Acc 0.850\n",
            "| Epoch  51 Iter  97 Time 1.261 Avg Time 1.272\tBatch Loss: 0.1919 Batch Acc 0.934\n",
            "| Epoch  51 Time 131.739 \tTest Acc 0.851\n",
            "| Epoch  52 Iter  97 Time 1.268 Avg Time 1.273\tBatch Loss: 0.1577 Batch Acc 0.945\n",
            "| Epoch  52 Time 132.296 \tTest Acc 0.849\n",
            "| Epoch  53 Iter  97 Time 1.265 Avg Time 1.271\tBatch Loss: 0.1512 Batch Acc 0.939\n",
            "| Epoch  53 Time 131.904 \tTest Acc 0.850\n",
            "| Epoch  54 Iter  97 Time 1.261 Avg Time 1.269\tBatch Loss: 0.1536 Batch Acc 0.951\n",
            "| Epoch  54 Time 131.401 \tTest Acc 0.849\n",
            "| Epoch  55 Iter  97 Time 1.261 Avg Time 1.271\tBatch Loss: 0.1872 Batch Acc 0.943\n",
            "| Epoch  55 Time 131.755 \tTest Acc 0.845\n",
            "| Epoch  56 Iter  97 Time 1.260 Avg Time 1.269\tBatch Loss: 0.1774 Batch Acc 0.932\n",
            "| Epoch  56 Time 131.496 \tTest Acc 0.843\n",
            "| Epoch  57 Iter  97 Time 1.258 Avg Time 1.268\tBatch Loss: 0.1594 Batch Acc 0.953\n",
            "| Epoch  57 Time 131.314 \tTest Acc 0.848\n",
            "| Epoch  58 Iter  97 Time 1.256 Avg Time 1.271\tBatch Loss: 0.1418 Batch Acc 0.947Epoch    58: reducing learning rate of group 0 to 1.0000e-03.\n",
            "\n",
            "| Epoch  58 Time 131.540 \tTest Acc 0.849\n",
            "| Epoch  59 Iter  97 Time 1.266 Avg Time 1.271\tBatch Loss: 0.1066 Batch Acc 0.951\n",
            "| Epoch  59 Time 131.981 \tTest Acc 0.854\n",
            "| Epoch  60 Iter  97 Time 1.259 Avg Time 1.268\tBatch Loss: 0.1349 Batch Acc 0.945\n",
            "| Epoch  60 Time 131.345 \tTest Acc 0.854\n",
            "| Epoch  61 Iter  97 Time 1.265 Avg Time 1.270\tBatch Loss: 0.0966 Batch Acc 0.969\n",
            "| Epoch  61 Time 131.921 \tTest Acc 0.854\n",
            "| Epoch  62 Iter  97 Time 1.261 Avg Time 1.270\tBatch Loss: 0.1064 Batch Acc 0.965\n",
            "| Epoch  62 Time 131.659 \tTest Acc 0.853\n",
            "| Epoch  63 Iter  97 Time 1.259 Avg Time 1.271\tBatch Loss: 0.0982 Batch Acc 0.965\n",
            "| Epoch  63 Time 131.756 \tTest Acc 0.853\n",
            "| Epoch  64 Iter  97 Time 1.263 Avg Time 1.268\tBatch Loss: 0.0948 Batch Acc 0.973\n",
            "| Epoch  64 Time 131.475 \tTest Acc 0.854\n",
            "| Epoch  65 Iter  97 Time 1.259 Avg Time 1.271\tBatch Loss: 0.0715 Batch Acc 0.973\n",
            "| Epoch  65 Time 131.718 \tTest Acc 0.851\n",
            "| Epoch  66 Iter  97 Time 1.261 Avg Time 1.269\tBatch Loss: 0.0710 Batch Acc 0.971\n",
            "| Epoch  66 Time 131.463 \tTest Acc 0.852\n",
            "| Epoch  67 Iter  97 Time 1.256 Avg Time 1.270\tBatch Loss: 0.0807 Batch Acc 0.971\n",
            "| Epoch  67 Time 131.606 \tTest Acc 0.852\n",
            "| Epoch  68 Iter  97 Time 1.258 Avg Time 1.271\tBatch Loss: 0.0799 Batch Acc 0.969Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
            "\n",
            "| Epoch  68 Time 131.733 \tTest Acc 0.852\n",
            "| Epoch  69 Iter  97 Time 1.261 Avg Time 1.271\tBatch Loss: 0.0622 Batch Acc 0.979\n",
            "| Epoch  69 Time 131.834 \tTest Acc 0.853\n",
            "| Epoch  70 Iter  97 Time 1.260 Avg Time 1.266\tBatch Loss: 0.0648 Batch Acc 0.975\n",
            "| Epoch  70 Time 131.343 \tTest Acc 0.853\n",
            "| Epoch  71 Iter  97 Time 1.261 Avg Time 1.269\tBatch Loss: 0.0458 Batch Acc 0.984\n",
            "| Epoch  71 Time 131.453 \tTest Acc 0.853\n",
            "| Epoch  72 Iter  97 Time 1.266 Avg Time 1.269\tBatch Loss: 0.0728 Batch Acc 0.971\n",
            "| Epoch  72 Time 131.662 \tTest Acc 0.853\n",
            "| Epoch  73 Iter  97 Time 1.262 Avg Time 1.267\tBatch Loss: 0.0645 Batch Acc 0.980\n",
            "| Epoch  73 Time 131.310 \tTest Acc 0.854\n",
            "| Epoch  74 Iter  97 Time 1.260 Avg Time 1.268\tBatch Loss: 0.0614 Batch Acc 0.975\n",
            "| Epoch  74 Time 131.716 \tTest Acc 0.853\n",
            "| Epoch  75 Iter  97 Time 1.260 Avg Time 1.270\tBatch Loss: 0.0586 Batch Acc 0.979Epoch    75: reducing learning rate of group 0 to 1.0000e-05.\n",
            "\n",
            "| Epoch  75 Time 131.642 \tTest Acc 0.853\n",
            "| Epoch  76 Iter  97 Time 1.258 Avg Time 1.272\tBatch Loss: 0.0550 Batch Acc 0.984\n",
            "| Epoch  76 Time 131.803 \tTest Acc 0.853\n",
            "| Epoch  77 Iter  97 Time 1.260 Avg Time 1.271\tBatch Loss: 0.0785 Batch Acc 0.971\n",
            "| Epoch  77 Time 131.560 \tTest Acc 0.853\n",
            "| Epoch  78 Iter  97 Time 1.264 Avg Time 1.269\tBatch Loss: 0.0682 Batch Acc 0.975\n",
            "| Epoch  78 Time 131.407 \tTest Acc 0.853\n",
            "| Epoch  79 Iter  97 Time 1.260 Avg Time 1.271\tBatch Loss: 0.0640 Batch Acc 0.977\n",
            "| Epoch  79 Time 131.517 \tTest Acc 0.853\n",
            "| Epoch  80 Iter  97 Time 1.259 Avg Time 1.270\tBatch Loss: 0.0335 Batch Acc 0.992\n",
            "| Epoch  80 Time 131.676 \tTest Acc 0.853\n",
            "| Epoch  81 Iter  97 Time 1.261 Avg Time 1.269\tBatch Loss: 0.0776 Batch Acc 0.973\n",
            "| Epoch  81 Time 131.565 \tTest Acc 0.853\n",
            "| Epoch  82 Iter  78 Time 1.265 Avg Time 1.274\tBatch Loss: 0.0399 Batch Acc 0.984"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n",
            "    send_bytes(obj)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n",
            "    self._send_bytes(m[offset:offset + size])\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n",
            "    self._send(header + buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n",
            "  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n",
            "    n = write(self._handle, buf)\n",
            "BrokenPipeError: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-e96023b033cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \"\"\"\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLxi0J4hBAf-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.empty_cache()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0J1kXSk8EOY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "outputId": "3851a640-3195-4ec4-f266-8d252aa8d64a"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun May 19 07:24:03 2019       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 418.67       Driver Version: 410.79       CUDA Version: 10.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   55C    P0    27W /  70W |   2703MiB / 15079MiB |      0%      Default |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                       GPU Memory |\n",
            "|  GPU       PID   Type   Process name                             Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQhIB4Cb-dr9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "outputId": "bc37680a-f75f-4f08-cdc2-4ff6abffcdc4"
      },
      "source": [
        "import time\n",
        "import torch\n",
        "\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.9,\n",
        "                      weight_decay=5e-4, nesterov=True)\n",
        "lr_schedule = lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=10, \n",
        "                                             verbose=True)\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "  batch_time = time.time()\n",
        "  epoch_time = batch_time\n",
        "  for iteration, (x, y) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    x, y = x.cuda(), y.cuda()\n",
        "    scores = model(x)\n",
        "    loss = loss_fn(scores, y)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "      _, predicted = torch.max(scores, 1)\n",
        "      train_acc = torch.eq(predicted, y).sum().item()/x.shape[0]\n",
        "\n",
        "      print('\\r| Epoch {:3d} Iter {:3d} Time {:.3f} Avg Time {:.3f}\\t'\n",
        "            'Batch Loss: {:.4f} Batch Acc {:.3f}'.format(\n",
        "                epoch, iteration+1, time.time()-batch_time, \n",
        "                (time.time()-epoch_time)/(iteration+1),\n",
        "                loss.item(), train_acc\n",
        "            ), end='')\n",
        "      batch_time = time.time()\n",
        "      del x, y, scores, loss, predicted\n",
        "  optimizer.zero_grad()\n",
        "  test_acc = acc(test_loader)\n",
        "  lr_schedule.step(test_acc)\n",
        "  print('\\n| Epoch {:3d} Time {:.3f} \\tTest Acc {:.3f}'.format(\n",
        "    epoch, time.time()-epoch_time, test_acc\n",
        "  ))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Epoch   0 Iter  97 Time 1.263 Avg Time 1.234\tBatch Loss: 0.4823 Batch Acc 0.824\n",
            "| Epoch   0 Time 128.153 \tTest Acc 0.785\n",
            "| Epoch   1 Iter   7 Time 1.261 Avg Time 1.336\tBatch Loss: 0.4643 Batch Acc 0.836"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-917da44e7949>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/optim/sgd.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    100\u001b[0m                         \u001b[0mbuf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mdampening\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mnesterov\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_p\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m                         \u001b[0md_p\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ts9SDGy-gQ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}